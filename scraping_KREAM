import pandas as pd
import os
import re
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

scrolls = 150
main_url = 'https://kream.co.kr/social/tags/sneakers'


def scrape_post(driver, post_url):
    try:
        print(f"Scraping post: {post_url}")
        driver.get(post_url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'full_width'))) 
        time.sleep(10)  # Add a short delay to ensure content is loaded

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        pagination = soup.find('div', {'class': 'flicking-pagination'})
        num_images = len(pagination.find_all('span', {'class': 'flicking-pagination-bullet'})) if pagination else 1

        image_elements = soup.find_all('picture', {'class': 'social_img'})[:num_images]
        image_urls = [img.find('img')['src'] for img in image_elements]

        caption = soup.find('div', {'class': 'social_text'}).get_text(strip=True)
        username = soup.find('span', {'class': 'user_name'}).get_text(strip=True)

        product_links = [f"https://kream.co.kr{link['href']}" for link in soup.find_all('a', {'class': 'product_link'})]
        product_links_str = ', '.join(product_links)

        return {
            'post_url': post_url,
            'image_urls': ', '.join(image_urls),
            'caption': caption,
            'username': username,
            'product_links': product_links_str
        }
    except Exception as e:
        print(f"Error scraping post {post_url}: {e}")
        return None

def scrape_main_page(driver, main_url, scroll_to_end=True, scroll_count=5):
    driver.get(main_url)
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'full_width'))) 

    print(f"Scraping main page: {main_url}")

    # store 
    visited_post_urls = set()

    if scroll_to_end:
        last_height = driver.execute_script("return document.body.scrollHeight")
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(10)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
    else:
        for _ in range(scroll_count):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(10)


    soup = BeautifulSoup(driver.page_source, 'html.parser')
    post_links = soup.find_all('a', href=re.compile('/social/posts/'))

    print(f"Number of post links found: {len(post_links)}")
    
    post_urls = []
    for link in post_links:
        post_url = f"https://kream.co.kr{link['href']}"
        if post_url not in visited_post_urls:
            visited_post_urls.add(post_url)
            post_urls.append(post_url)

    return post_urls


driver = webdriver.Chrome()

post_urls = scrape_main_page(driver, main_url, scroll_to_end=False, scroll_count=scrolls)
all_data = []

for url in post_urls:
    post_data = scrape_post(driver, url)
    if post_data:
        all_data.append(post_data)
        print(f"Successfully appended {url}")
    else:
        print(f"Failed to scrape {url}")
